{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmsmySMss6ni"
   },
   "source": [
    "# **Ajuste fino de DistilBERT para Tareas de Elección Múltiple**\n",
    "\n",
    "**Autor:** Jorge Elías García  \n",
    "**Correo:** [jorge.elias@alumnos.upm.es](mailto:jorge.elias@alumnos.upm.es)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kwmq_ryNWjDn"
   },
   "source": [
    "## **Introducción**\n",
    "\n",
    "En este cuaderno se detallará el proceso de seleccionar, configurar y ejecutar el ajuste\n",
    "fino (fine-tuning) de un Modelo de Lenguaje (LM) preentrenado para una tarea específica de Procesamiento del\n",
    "Lenguaje Natural (NLP). Concretamente, se realizará un **fine-tunning para *Multiple Choice* empleando DistilBERT como LLM preentrenado**.\n",
    "\n",
    "En primer lugar, abordamos una tarea de **NLP de elección múltiple**, concretamente un problema en el que el modelo debe seleccionar la continuación más coherente de un contexto entre varias opciones posibles. Este tipo de tareas exige que el modelo no solo comprenda el contenido explícito del texto, sino que también sea capaz de razonar sobre relaciones causales, semánticas y de sentido común. Para ello utilizamos el dataset **SWAG**, un conjunto de ejemplos diseñado específicamente para evaluar la capacidad de los modelos de lenguaje en tareas de razonamiento situacional (*commonsense reasoning*).\n",
    "\n",
    "En segundo lugar, para esta tarea se ha elegido `distilbert-base-uncased` como modelo de lenguaje base preentrenado, ya que se trata de una versión eficiente y eficaz de la familia BERT, manteniendo un **rendimiento similar al de BERT**, pero con **menos parámetros**, lo que conlleva tiempos de entrenamiento más reducidos. Asimismo, al ser una arquitectura *encoder-only*, está específicamente diseñada para tareas de **comprensión** y **razonamiento** sobre el texto, lo que resulta especialmente adecuado para el conjunto **SWAG**, donde el modelo debe analizar un contexto y determinar cuál de las opciones es la continuación más plausible. Por estos motivos, se ha considerado **DistilBERT** como la arquitectura *encoder-only*  más práctica y equilibrada para llevar a cabo el *fine-tuning* en una tarea de *multiple choice* sin sacrificar calidad en la predicción.\n",
    "\n",
    "\n",
    "En último lugar, dado que `distilbert-base-uncased` no está entrenado para tareas de *multiple choice*, es necesario emplear la técnica de **ajuste fino (fine-tuning)** sobre SWAG. El proceso de ajuste fino consiste en adaptar DistilBERT a esta tarea mediante varios pasos encadenados: la carga y tokenización del dataset SWAG, la preparación del modelo con una cabeza específica de *multiple choice*, la definición de los hiperparámetros de entrenamiento, la implementación de una función para la evaluación del modelo y, finalmente, el entrenamiento supervisado junto con la evaluación periódica del rendimiento. Este flujo permite especializar el modelo en seleccionar la continuación correcta entre varias opciones a partir del contexto proporcionado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iz7ZzCXxwoX5"
   },
   "source": [
    "## **Carga de las librerías necesarias y comprobación de GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcS_JHVKX_pO"
   },
   "source": [
    "En primer lugar, cargamos las librerías necesarias. Incorporaremos:\n",
    "\n",
    "* `transformers`: Colección de modelos de lenguaje preentrenados (e.g., BERT) para tareas de NLP. Además de herramientas necesarias, como el tokenizador.\n",
    "\n",
    "* `datasets`: Acceso simple a grandes datasets.\n",
    "\n",
    "* `evaluate`: Herramienta que ofrece las métricas esenciales (como accuracy) para evaluar el rendimiento del modelo.\n",
    "\n",
    "* `accelerate`: Permite un entrenamiento rápido y optimizado en diferentes configuraciones de hardware (CPU/GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1_6yWikTWIct",
    "outputId": "71cafe1b-0739-40ac-c215-00cc102a2f12"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets accelerate evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEqQsiKv5oeN"
   },
   "source": [
    "Comprobamos también la disponibilidad de GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zai78xXM5rxZ",
    "outputId": "5fce0a7c-aa10-42a6-8fe9-7c9b62d572c8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU detected.\")\n",
    "    print(\"If you are using Google Colab, please go to 'Runtime' > 'Change runtime type' and select 'GPU' as the hardware accelerator.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqyLZCcqZCuN"
   },
   "source": [
    "## **Dataset**\n",
    "\n",
    "El dataset `swag` (*Situations With Adversarial Generations*) es un corpus diseñado para evaluar **razonamiento de sentido común** mediante tareas de **selección múltiple**. Cada ejemplo contiene un breve contexto dividido en dos partes (`sent1` y `sent2`) y **cuatro posibles continuaciones** (`ending0`, `ending1`, `ending2`, `ending3`). El objetivo del modelo consiste en predecir cuál de estas opciones constituye la **continuación más plausible** según el sentido común.\n",
    "\n",
    "Usamos la librería `datasets` para descargar el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HVk93GTdaMCF",
    "outputId": "bd837b3d-d403-4b57-c302-64c9db0889a8"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Cargamos la versión estándar (\"regular\") del dataset SWAG.\n",
    "dataset = load_dataset(\"swag\", \"regular\")\n",
    "\n",
    "# Comprobamos el tamaño de los conjuntos de entrenamiento y validación.\n",
    "print(\"Train:\", len(dataset[\"train\"]))\n",
    "print(\"Validation:\", len(dataset[\"validation\"]))\n",
    "print(\"Test:\", len(dataset[\"test\"]))\n",
    "\n",
    "# Mostramos un ejemplo.\n",
    "ej = dataset[\"train\"][0]\n",
    "print(\"sent1:\", ej[\"sent1\"])\n",
    "print(\"sent2:\", ej[\"sent2\"])\n",
    "print(\"\\nOpciones:\")\n",
    "for i in range(4):\n",
    "    print(f\"  ending{i}:\", ej[f\"ending{i}\"])\n",
    "print(\"\\nLabel correcto:\", ej[\"label\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVeLg3LMxNMx"
   },
   "source": [
    "Definimos desde ya el nombre del modelo, pues será necesario para el tokenizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2HukgTSMR15V"
   },
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpPC7elEywZK"
   },
   "source": [
    "### **Preparación del dataset**\n",
    "\n",
    "Para preparar los datos del conjunto **SWAG** se utiliza la clase `AutoTokenizer`, que permite cargar automáticamente el tokenizador específico del modelo preentrenado. Esto asegura que el texto se procese con el **mismo vocabulario y las mismas reglas de segmentación** que el modelo escogido, manteniendo la coherencia entre los tokens generados y los embeddings que el modelo espera recibir.\n",
    "\n",
    "La tokenización implica **dividir el texto en subpalabras o tokens**, convertir esos tokens en **índices numéricos**, y unificar la longitud de todas las secuencias mediante **padding** o **truncación** cuando es necesario. Además, se añaden los **tokens especiales** propios del modelo y se construyen **máscaras de atención** que distinguen entre contenido real y posiciones de relleno. Con esta representación numérica uniforme, el modelo puede interpretar correctamente cada par *(contexto, opción)*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246,
     "referenced_widgets": [
      "337e05b71dbc40e1887711761330db32",
      "6c746807e1e5417eb90a17803eeaf829",
      "87cfd25b9ea7408eb1868b947b1cc696",
      "71899303cbbe4ed68d797490c2520fc9",
      "44de57cffb4c4303a5f4ec7610e30b29",
      "6502e759039a4f568846d30d72893def",
      "dbe755af62a3472faeb4f2676fa6d4bd",
      "7dd6b024c2254757a08ea6862b91ba1a",
      "1edb39178ac64aa2b31833c02d1588ae",
      "47b1d2a9420d46a8ae963707b5f3a5c6",
      "3ff8be46252f452689a962360e69eedf",
      "4fd5964676a74b2ca552a694c054abd1",
      "2b121f126e1948b89a152a3537b5def0",
      "0911aa5b38be43238a771cff2c853bf6",
      "5b1e79c2adec41bbba964bd087ec7ec0",
      "74347993f8a04f629c84c566142c6c20",
      "bcb6795d2268472da51efdc26ed118e6",
      "8897a8fc3ad544f4b3318b1fe2e2d415",
      "ff7e7195e7394ab995284cb364d28bf1",
      "0b77fff28bf14de6be43b3d9e7e68aad",
      "575fa6e9642b4e08a80e8dcff0ab4c52",
      "27d607e533e549139d82ccea7e0567c9",
      "3e5d18f63daa4cf5bcbb174ccb00804e",
      "91c29ca8b5cb4ea7a8a463569e77d281",
      "15180bc675094d0caf7055e1432479db",
      "9f58b04992704a3da62b17b487a6d6ca",
      "ac388dfca6db4bce9d4ebe935f6f33b3",
      "62ff8418d7ec4fe3a19eaafc219f9486",
      "460f230317d746498c571b8541214fa1",
      "15be00f3e4004bf4979bc43438c819b2",
      "09961ba959e74aa9a1b15b3fa2399ee4",
      "eefbcbea86fc4f4a918cc487f13345f2",
      "088294f0b62044d9a9df22bdc8f5735b",
      "ed6f34003d904f36bbbf56e2ba6eac31",
      "f9104c3e48e149ec81c6b01cb8ce1160",
      "53e6941599334e2c85f6adb9752b0a48",
      "6ae84758157a4d668fe64a23b57b5eda",
      "f67cfeadec7546eb986d88aaf45c3b91",
      "1162c4647538453592352d2967b839d4",
      "9b3464c31ef547c5858763bae0c48a92",
      "1fc7f2e60adc455882dcc55eb89c08e2",
      "504ef62cc4a04a0283f2ea45e9ba739f",
      "586eef489526498da29133e13de0648e",
      "7fa5e91082944b5f84f7f3563dfe749c",
      "ffaae9ef8a0342999bcfab3176419e3a",
      "d39d65279ce24af689390bf1b1e634a8",
      "f04753af599c441097636b5d262304b9",
      "edca24cfe9b44c04aacf7331d0dd07a6",
      "197e0835bfcb4da39559d4593d59d244",
      "02e9e10cfeb244c29cfd2a052e404198",
      "67705d020e06483b9efeaedcac79c29a",
      "bddb8c9bc85949daab3754a9f0d8a250",
      "c460b9afdbcf46328c61b0f770fd9fe8",
      "f8099349feb74de2bf3e82b2d546df27",
      "403cca421aba4d47afc7f7937355ed26",
      "9a5bd947cb474829b2f762e4e0f4bad9",
      "8646ed2ca5b846d4bc0eaccc563ddbce",
      "9ab6da4cacfa4632a133fbaf6edd440f",
      "9caa05b0906749f1ae94adeb0f9a7605",
      "808eb50fa71d40f398722a69a5cc4480",
      "4875d16ac5f84ff6b692aab4791c2eca",
      "df3f2e1e7997433c87bc77386c79ca4c",
      "54b41870936c427e8861c92ed47a26e1",
      "267a694a806b4de6aea26bdbb5929233",
      "1de0ba5931084909ae6732e28377bfcd",
      "9c50bb77eba34c3298cf6dcb7b42684d"
     ]
    },
    "id": "nm_purj0y-4n",
    "outputId": "926c41a7-f006-49e9-ce73-cea9e92f6922"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Descargamos el tokenizador.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Función de preprocesado / tokenización para Multiple Choice.\n",
    "def preprocess_function(examples):\n",
    "    # Construimos el contexto concatenando sent1 + sent2.\n",
    "    contexts = [s1 + \" \" + s2 for s1, s2 in zip(examples[\"sent1\"], examples[\"sent2\"])]\n",
    "\n",
    "    # SWAG tiene 4 opciones por ejemplo.\n",
    "    choices = list(zip(\n",
    "        examples[\"ending0\"],\n",
    "        examples[\"ending1\"],\n",
    "        examples[\"ending2\"],\n",
    "        examples[\"ending3\"]\n",
    "    ))\n",
    "\n",
    "    # Aplanamos listas para tokenizar (contexto repetido 4 veces por ejemplo).\n",
    "    contexts_flat = []\n",
    "    choices_flat = []\n",
    "\n",
    "    for ctx, four_endings in zip(contexts, choices):\n",
    "        for ending in four_endings:\n",
    "            contexts_flat.append(ctx)\n",
    "            choices_flat.append(ending)\n",
    "\n",
    "    # Tokenización batch con el tokenizador de DistilBERT.\n",
    "    tokenized = tokenizer(\n",
    "        contexts_flat,\n",
    "        choices_flat,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "    # Volver a agrupar a forma (batch_size, num_choices=4, seq_len).\n",
    "    num_choices = 4\n",
    "    result = {}\n",
    "    for key, val in tokenized.items():\n",
    "        # val es una lista plana; la rehacemos en bloques de 4.\n",
    "        result[key] = [val[i:i+num_choices] for i in range(0, len(val), num_choices)]\n",
    "\n",
    "    # Añadimos las etiquetas\n",
    "    result[\"labels\"] = examples[\"label\"]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Creamos subconjuntos pequeños en texto y posteriormente tokenizamos solo esos subcojuntos, para mayor eficiencia.\n",
    "small_train_raw = dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_raw  = dataset[\"validation\"].shuffle(seed=42).select(range(300))\n",
    "small_train_dataset = small_train_raw.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=small_train_raw.column_names  # limpia columnas originales.\n",
    ")\n",
    "\n",
    "small_eval_dataset = small_eval_raw.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=small_eval_raw.column_names\n",
    ")\n",
    "\n",
    "# Imprimimos un ejemplo ya tokenizado.\n",
    "print(small_train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-kaeeqaxzgE"
   },
   "source": [
    "## **Implementación**\n",
    "\n",
    "A continuación, se detalla el procedimiento de carga del LM, detalles de los hiperparámetros de entrenamiento y ejecución del ajuste fino."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3FqrSPx0Wfc"
   },
   "source": [
    "### **Reemplazamos la cabeza del modelo**\n",
    "\n",
    "Para adaptar *DistilBERT* a la tarea de *multiple choice* del dataset **SWAG**, sustituimos su cabeza original por una cabeza especializada para la selección múltiple mediante la clase `AutoModelForMultipleChoice` de la librería **Transformers** (HuggingFace). Esta variante del modelo añade una capa final diseñada específicamente para puntuar varias opciones simultáneamente. En nuestro caso, indicamos `num_labels=4` porque cada ejemplo de SWAG contiene exactamente **cuatro posibles continuaciones del contexto**.\n",
    "\n",
    "Esta cabeza toma las representaciones generadas por DistilBERT para cada par *(contexto, opción)*, calcula un **logit** independiente para cada una de ellas y selecciona como predicción la opción con puntuación más alta. De este modo, en lugar de asignar una etiqueta fija a un texto, el modelo compara opciones entre sí bajo un mismo contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "c5f2206061694126841f501ba52b3776",
      "d314c81acf964291bd3a49d35731d6b1",
      "3a2987d929a24b6ba899a1d0d9c50181",
      "893344c015a146e8a528e6cbd4280102",
      "7e80d6cfdccc4d4bbba0fb83d7e55c78",
      "c651f19709354d7b8091e7efe92cab9c",
      "7c42630045174c3599cc231daa4edef0",
      "924c01402f404187b9a71e985ebbe07f",
      "110a8cf85071434589e96f34389479d7",
      "a058f42edf7d4f23b91a467da822d618",
      "cb72df460e6344cda052d7da34a68726"
     ]
    },
    "id": "ZpCMZAEr0cSA",
    "outputId": "210ae4d4-c2ff-4657-f3f3-4adf9865f499"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMultipleChoice\n",
    "\n",
    "# Cargamos el modelo con cabeza de multiple choice (4 opciones en SWAG).\n",
    "model = AutoModelForMultipleChoice.from_pretrained(model_name, num_labels=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEvMNsqQ062-"
   },
   "source": [
    "### **Seleccionamos los hiperparámetros para el entrenamiento**\n",
    "\n",
    "Para controlar el proceso de *fine-tuning*, empleamos la clase `TrainingArguments` de la librería **Transformers** (HuggingFace). Esta interfaz nos permite definir los principales hiperparámetros del entrenamiento, que luego serán utilizados por el `Trainer`.\n",
    "\n",
    "Entre los parámetros que se pueden configurar se encuentran:\n",
    "\n",
    "- **`eval_strategy`**  \n",
    "  Determina cuándo se realiza la evaluación del modelo (por ejemplo, por épocas o por pasos).\n",
    "\n",
    "- **`learning_rate`**  \n",
    "  Controla la magnitud de las actualizaciones de los pesos durante el entrenamiento.\n",
    "\n",
    "- **`num_train_epochs`**  \n",
    "  Indica cuántas veces el modelo recorre completamente el conjunto de entrenamiento.\n",
    "\n",
    "- **`weight_decay`**  \n",
    "  Forma de regularización que penaliza pesos demasiado grandes para evitar sobreajuste.\n",
    "\n",
    "- **`report_to`**  \n",
    "  Especifica a qué herramientas externas se envían los registros de entrenamiento. En nuestro caso lo desactivamos (`\"none\"`) para evitar cualquier integración automática con plataformas como Weights & Biases, lo cual impide que durante el entrenamiento aparezcan avisos pidiendo claves o configuraciones externas que no necesitamos.\n",
    "\n",
    "\n",
    "Estos han sido los hiperparámetros seleccionados. No obstante, existen otras muchas variables con posibilidad de configuración, para de esta manera adaptar el entrenamiento a los requerimientos específicos del usuario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "neOG3XF51BTm"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    eval_strategy=\"epoch\", # La evaluación del modelo se hará al acabar cada época.\n",
    "    learning_rate=2e-5, # Damos un valor bajo al learning rate, para evitar cambios muy bruscos u olvido catastrófico.\n",
    "    num_train_epochs=3, # Entrenamos durante 3 épocas, para prevenir el olvido catastrófico.\n",
    "    weight_decay=0.01, # Penalizamos los pesos muy grandes para evitar sobreajuste.\n",
    "    report_to=\"none\" # Descativamos los loggings externos.\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGi61UMp1G6q"
   },
   "source": [
    "### **Creamos una función para la evaluación del modelo**\n",
    "\n",
    "El `Trainer` no calcula métricas por sí mismo, por lo que necesitamos definir una función externa que indique cómo evaluar el rendimiento del modelo. Para ello utilizamos la librería **Evaluate** de HuggingFace, que ofrece implementaciones listas para usar de métricas habituales, como la *accuracy*.\n",
    "\n",
    "Primero cargamos la métrica con `evaluate.load`, y luego definimos una función `compute_metrics` que el `Trainer` llamará automáticamente cada vez que corresponda realizar una evaluación. Esta función recibe las predicciones del modelo y las etiquetas reales, extrae la clase más probable en cada ejemplo y devuelve el valor de la métrica seleccionada.\n",
    "\n",
    "La frecuencia con la que esta evaluación ocurre depende del parámetro `evaluation_strategy` definido en `TrainingArguments`, que puede indicar, por ejemplo, que la evaluación se realice al finalizar cada época (como en nuestro caso)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "d91e4e2a4f2344e2ae0134454b9a0726",
      "d951b872bd1a42e69b426277c353e323",
      "42dcc0d96a4944c1b3a3dbf991dbaf4c",
      "32f7e0d8933649c9acf85e9ce4e7cf62",
      "d1026f3567564b54a4d4664ac4957038",
      "b74403d6bc554de3a0946841e69bf24a",
      "583703d23f6846dd9f31072c0f1076c9",
      "8a0111b933ea4392b98d74cea186382b",
      "a2666aef95744d76bd12d24cecd78bf3",
      "3c7aba50c9e442f885fca23a6a4023c6",
      "961b095612ba4b26b5fdc991bb001e04"
     ]
    },
    "id": "Pwl_DuOH1NWw",
    "outputId": "105a3a8d-f9a4-492e-c9bb-58a75868b5bf"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# Métrica de accuracy.\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSaYQhQQ1NvM"
   },
   "source": [
    "### **Entrenamos y evaluamos el modelo**\n",
    "\n",
    "Finalmente, podemos hacer uso de la clase `Trainer` para llevar a cabo el entrenamiento del modelo. Este componente unifica todo el proceso de entrenamiento y evaluación, permitiendo ejecutar el pipeline completo de forma automática.\n",
    "\n",
    "El `Trainer` recibirá:  \n",
    "- **`model`**: el modelo ya configurado con la cabeza de *multiple choice*.  \n",
    "- **`args`**: los hiperparámetros definidos previamente mediante `TrainingArguments`.  \n",
    "- **`train_dataset` y `eval_dataset`**: los subconjuntos de entrenamiento y validación ya preprocesados.  \n",
    "- **`compute_metrics`**: la función encargada de calcular las métricas durante la evaluación.\n",
    "\n",
    "Una vez inicializado, simplemente llamaremos a `trainer.train()` para iniciar el proceso de *fine-tuning* de principio a fin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "66Gvg45f1TnJ",
    "outputId": "2925665e-d5b7-47d4-af24-a5e4afb58692"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Empleamos todos los parámetros creados previamente.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-XWM2DAyLux"
   },
   "source": [
    "## **Resultados y Discusión**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHGvtNXzbGsu"
   },
   "source": [
    "⚠️ **NOTA:** Es posible que la tabla con la actualización de la función de pérdida y accuracy por época no aparezca en el cuaderno descargado, debido a que los logs del entrenamiento no se guardan como salida de celda. No obstante, en archivo PDF adjunto a la tarea, sí aparece reflejada dicha tabla.\n",
    "\n",
    "A lo largo del proceso de *fine-tuning*, el modelo muestra una evolución progresiva tanto en la pérdida de validación como en la métrica de *accuracy*. Aunque el conjunto de datos empleado es relativamente reducido, se aprecia una mejora estable entre épocas. En la tercera y última época, el modelo alcanza aproximadamente un **50% de acierto**, lo que refleja una capacidad razonable para seleccionar continuaciones coherentes en la tarea de *multiple choice*, especialmente teniendo en cuenta la complejidad semántica del dataset SWAG y el tamaño limitado del subconjunto utilizado.\n",
    "\n",
    "Es importante considerar que modelos como DistilBERT pueden **sobreajustarse rápidamente** cuando trabajan con cantidades pequeñas de datos. Incrementar el número de épocas podría mejorar temporalmente el rendimiento, pero también aumenta el riesgo de *overfitting* o incluso de **olvido catastrófico**, donde el modelo pierde parte del conocimiento adquirido durante su preentrenamiento al especializarse en exceso en la tarea actual. Por este motivo no ampliamos el número de épocas: mantener el entrenamiento acotado ayuda a evitar que el modelo se sobreajuste o pierda parte del conocimiento adquirido durante el preentrenamiento, preservando así un equilibrio adecuado entre aprendizaje y generalización.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6bNrDBy9gGS"
   },
   "source": [
    "### **Predicción con el modelo ajustado**\n",
    "\n",
    "Para evaluar el modelo sobre ejemplos personalizados de *multiple choice* seguimos la misma lógica que durante el entrenamiento, pero aplicada a nuevos datos. Es fundamental reutilizar **el mismo tokenizador** y **el mismo modelo entrenado**, de modo que el texto se procese con el mismo vocabulario, los mismos tokens especiales y el mismo formato de entrada que el modelo ha visto durante el *fine-tuning*.\n",
    "\n",
    "Cada ejemplo se construye combinando un **mismo contexto** con sus **cuatro opciones de respuesta**, que el tokenizador transforma en pares `(contexto, opción)` y convierte en tensores listos para el modelo. Para cada una de estas secuencias, DistilBERT genera una representación contextualizada (*embedding*) que resume la información del par completo. Estas cuatro representaciones llegan a la cabeza de *multiple choice*, que calcula un **logit** para cada opción. Finalmente, el modelo devuelve los loggits y seleccionamos la respuesta con la puntuación más alta, equivalente a la opción que el modelo considera más probable.\n",
    "\n",
    "Ejecutamos `model.eval()` al comienzo de la celda, para cerciorarnos de que el modelo se encuentra en modo inferencia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OxaDiY6X-hAT"
   },
   "outputs": [],
   "source": [
    "model.eval() # Aseguramos de que el modelo esté en modo de evaluación.\n",
    "\n",
    "def predict_multiple_choice(custom_sent1, custom_sent2, custom_options, tokenizer, model, device, max_length=64):\n",
    "    \"\"\"\n",
    "    Realizamos predicciones sobre ejemplos personalizados de multiple choice.\n",
    "\n",
    "    custom_sent1: primera oración del contexto.\n",
    "    custom_sent2: segunda oración del contexto.\n",
    "    custom_options: lista de listas, cada sublista contiene las 4 opciones.\n",
    "    tokenizer: tokenizador del modelo.\n",
    "    model: modelo entrenado.\n",
    "    device: \"cuda\" o \"cpu\".\n",
    "    max_length: longitud máxima de tokenización.\n",
    "    \"\"\"\n",
    "\n",
    "    # Preparamos listas planas para tokenización (igual que en preprocess_function).\n",
    "    contexts_flat = []\n",
    "    choices_flat = []\n",
    "\n",
    "    for s1, s2, opts in zip(custom_sent1, custom_sent2, custom_options):\n",
    "        ctx = s1 + \" \" + s2\n",
    "        for opt in opts:\n",
    "            contexts_flat.append(ctx)\n",
    "            choices_flat.append(opt)\n",
    "\n",
    "    # Tokenizamos.\n",
    "    tokenized = tokenizer(\n",
    "        contexts_flat,\n",
    "        choices_flat,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Reagrupamos a forma (batch_size, num_choices, seq_len).\n",
    "    num_choices = 4\n",
    "    input_ids = tokenized[\"input_ids\"].view(-1, num_choices, tokenized[\"input_ids\"].size(-1))\n",
    "    attention_mask = tokenized[\"attention_mask\"].view(-1, num_choices, tokenized[\"attention_mask\"].size(-1))\n",
    "\n",
    "    inputs = {\n",
    "        \"input_ids\": input_ids.to(device),\n",
    "        \"attention_mask\": attention_mask.to(device)\n",
    "    }\n",
    "\n",
    "    # Realizamos la inferencia.\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=-1).tolist()\n",
    "\n",
    "    # Mostramos los resultados.\n",
    "    for i, pred_idx in enumerate(preds):\n",
    "        print(f\"\\n===== EJEMPLO {i} =====\")\n",
    "        print(\"Contexto:\", custom_sent1[i], custom_sent2[i])\n",
    "        print(\"\\nOpciones:\")\n",
    "        for j, opt in enumerate(custom_options[i]):\n",
    "            marcador = \"  <-- PREDICHA\" if j == pred_idx else \"\"\n",
    "            print(f\"  [{j}] {opt}{marcador}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0dZ2133BoKO"
   },
   "source": [
    "### **1. Predicción de ejemplos sencillos**\n",
    "\n",
    "En primer lugar, estudiamos el rendimiento del modelo con un conjunto de **ejemplos más sencillos.** En teoría, los ejemplos deberían ser suficientemente fáciles para que el modelo identifique claramente la opción que completa el contexto de manera más coherente. Consecuentemente, comprobamos si el modelo ha aprendido el patrón general de la tarea y es capaz de escoger la continuación más lógica antes de pasar a casos más complejos o ambiguos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "msITdG6OBqLm",
    "outputId": "f96620e7-fc8e-4de2-c584-9de70f04460a"
   },
   "outputs": [],
   "source": [
    "# Ejemplos personalizados.\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Si es posible empleamos la GPU.\n",
    "\n",
    "custom_sent1 = [\n",
    "    \"He dropped the glass on the floor\",\n",
    "    \"During her trip to Tenerife\"\n",
    "]\n",
    "\n",
    "custom_sent2 = [\n",
    "    \"and then he\",\n",
    "    \"she decided to\"\n",
    "]\n",
    "\n",
    "custom_options = [\n",
    "    [  # opciones para el ejemplo 0\n",
    "        \"picked it up carefully.\",\n",
    "        \"watched it fly away into the sky.\",\n",
    "        \"ignored the loud music.\",\n",
    "        \"answered the phone.\"\n",
    "    ],\n",
    "    [  # opciones para el ejemplo 1\n",
    "        \"prepare a snowman in the icy mountains.\",\n",
    "        \"ski across the frozen valley.\",\n",
    "        \"visit the beach on the sunny coast.\",\n",
    "        \"explore the deep Arctic glacier.\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "predict_multiple_choice(\n",
    "    custom_sent1,\n",
    "    custom_sent2,\n",
    "    custom_options,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2LIk1Ujd8UC"
   },
   "source": [
    "Apreciamos que el modelo ha **razonado correctamente**, prediciendo la continuación más lógica para ambos ejemplos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iiv9Br1B0Ee"
   },
   "source": [
    "### **2. Predicción de ejemplos más complejos**\n",
    "\n",
    "\n",
    "\n",
    "Tras verificar el comportamiento del modelo en situaciones sencillas, analizamos cómo se comporta ante ejemplos menos directos o con opciones más similares entre sí. Este tipo de casos pone a prueba la capacidad del modelo para captar matices semánticos, resolver ambigüedades y seleccionar la opción más coherente cuando el contexto no ofrece una pista evidente. Evaluar ejemplos más complejos nos permite comprobar si el *fine-tuning* ha dotado realmente al modelo de una comprensión más profunda de la tarea, más allá de los patrones triviales.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8PANMmfEB16b",
    "outputId": "d5bbb1cd-3007-4014-d4be-b1d43232a5e5"
   },
   "outputs": [],
   "source": [
    "# Ejemplos personalizados más complejos.\n",
    "custom_sent1 = [\n",
    "    \"After running for two hours in the rain\",\n",
    "    \"The child looked at the strange device on the table\",\n",
    "    \"When the rocket finally launched into the sky\",\n",
    "    \"After landing in Tenerife for the first time\"\n",
    "]\n",
    "\n",
    "custom_sent2 = [\n",
    "    \"he finally\",\n",
    "    \"and slowly\",\n",
    "    \"the scientists in the control room\",\n",
    "    \"she looked around and\"\n",
    "]\n",
    "\n",
    "custom_options = [\n",
    "    [  # Ejemplo 0\n",
    "        \"felt a wave of exhaustion hit him.\",\n",
    "        \"started preparing a large meal.\",\n",
    "        \"painted the walls of his house blue.\",\n",
    "        \"won the lottery unexpectedly.\"\n",
    "    ],\n",
    "    [  # Ejemplo 1\n",
    "        \"reached for his backpack to go home.\",\n",
    "        \"took a step back, unsure of what it did.\",\n",
    "        \"flew out the window like a bird.\",\n",
    "        \"began singing loudly in the street.\"\n",
    "    ],\n",
    "\n",
    "    [  # Ejemplo 2\n",
    "        \"watched with excitement as data streamed in.\",\n",
    "        \"baked a cake to celebrate the event.\",\n",
    "        \"took a nap on the floor.\",\n",
    "        \"cleaned the windows in the building.\"\n",
    "\n",
    "    ],\n",
    "    [  # Ejemplo 3\n",
    "        \"admired the volcanic landscape and warm breeze.\",\n",
    "        \"joined a high-speed car race across the mountains.\",\n",
    "        \"painted a huge mural inside the airplane cabin.\",\n",
    "        \"went directly to the Eiffel Tour.\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "predict_multiple_choice(\n",
    "    custom_sent1,\n",
    "    custom_sent2,\n",
    "    custom_options,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTW9NAItp9-X"
   },
   "source": [
    "A partir de estos ejemplos más complejos, observamos que el modelo tiende a seleccionar opciones **coherentes con el contexto**, incluso cuando la respuesta correcta no es trivial. En el primer caso, aunque **no elige la opción que refleja de forma más directa el cansancio físico tras correr bajo la lluvia, sí selecciona una continuación razonable dentro de la situación**, lo que indica que mantiene cierta consistencia semántica. En los demás ejemplos, el modelo identifica correctamente la opción que mejor encaja: se aleja de alternativas absurdas o incompatibles y prefiere aquellas que guardan una relación lógica con la escena descrita, como la reacción cautelosa ante un dispositivo desconocido, el seguimiento de datos en un lanzamiento de cohete o la referencia al paisaje volcánico y clima cálido en Tenerife. En conjunto, estos resultados sugieren que **el modelo ha aprendido a utilizar el contexto para filtrar opciones y priorizar continuaciones plausibles.**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBMeolyfy6Zc"
   },
   "source": [
    "### **Evaluación del rendimiento y limitaciones**\n",
    "\n",
    "El modelo muestra un comportamiento razonablemente sólido tras el ajuste fino. En los ejemplos sencillos, donde la relación entre el contexto y la opción correcta es directa, el modelo no comete errores y selecciona siempre la continuación esperada. En los ejemplos más complejos —aquellos con alternativas plausibles o ambiguas— el modelo mantiene una elección “coherente”, evitando opciones incompatibles o sin sentido. Aunque no siempre identifica la mejor respuesta, sí tiende a elegir opciones que encajan semánticamente con la situación descrita.\n",
    "\n",
    "No obstante, las limitaciones también se hacen evidentes. Al evaluar el modelo sobre el conjunto completo de validación de SWAG, su precisión se sitúa en torno al **50%**, lo que indica a pensar que, con frecuencia, selecciona respuestas razonables pero no necesariamente la más adecuada. Esto refleja la dificultad inherente de la tarea, el tamaño reducido del conjunto utilizado para el entrenamiento y la propia capacidad de DistilBERT, que puede quedarse corto para capturar matices más sutiles de razonamiento de sentido común. En conjunto, aunque el modelo muestra un desempeño aceptable, aún queda margen de mejora para alcanzar una comprensión más fina del contexto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2bCB1_-qAK7"
   },
   "source": [
    "## **Conclusión**\n",
    "\n",
    "En este trabajo hemos llevado a cabo el *fine-tuning* de **DistilBERT** para una tarea de *multiple choice* utilizando el dataset **SWAG**, siguiendo todas las etapas necesarias para este proceso: preprocesamiento del conjunto de datos, adaptación del modelo mediante la cabeza `AutoModelForMultipleChoice`, configuración estructurada de los hiperparámetros con `TrainingArguments`, integración de una función de evaluación personalizada y entrenamiento supervisado con el `Trainer`. Posteriormente, evaluamos el comportamiento del modelo tanto en ejemplos sencillos como en casos más complejos y ambiguos para valorar su capacidad de razonamiento contextual.\n",
    "\n",
    "Los resultados muestran que, tras el *fine-tuning*, el modelo es capaz de **seleccionar continuaciones razonables en muchos casos, evitando opciones claramente incompatibles y manteniendo cierta coherencia contextual en ejemplos aislados**. Sin embargo, al evaluar su rendimiento en el propio conjunto SWAG, el modelo no supera el **50% de acierto**, lo que indica que tiene dificultades para identificar sistemáticamente la opción más adecuada cuando varias alternativas son plausibles. Esta limitación sugiere que la tarea plantea un nivel de complejidad que DistilBERT, incluso ajustado, no siempre logra resolver, especialmente en situaciones donde se requiere un razonamiento más fino o conocimiento implícito de sentido común.\n",
    "\n",
    "\n",
    "\n",
    "Finalmente, este proceso pone de manifiesto la flexibilidad que ofrece la librería **Transformers** y el uso del **Trainer** basado en PyTorch. El *fine-tuning* completo puede implementarse con muy pocas líneas de código, pero al mismo tiempo permite un ajuste totalmente personalizado del modelo sobre **nuestros propios datos**, a diferencia del uso directo de modelos ya entrenados mediante la función `pipeline`. Asimismo, probar diferentes arquitecturas es muy ágil: cambiar de DistilBERT a RoBERTa o a cualquier otro modelo solo requiere ajustar muy pocas líneas de código, lo que facilita evaluar de forma inmediata cómo varía el rendimiento entre modelos.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
